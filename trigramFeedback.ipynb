{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of feedbacks in the dataset:  8113\n",
      "First feedback:  slide giáo trình đầy đủ .\n",
      "Last feedback:  em có học ở một trung tâm tiếng anh ở ngoài trường , thời lượng hợp lý , không nhiều không ít khoảng 2 - 3 tiết một buổi !\n"
     ]
    }
   ],
   "source": [
    "# Load in data\n",
    "with open(\"student_feedback_dataset.txt\", 'r', encoding='utf8') as file:\n",
    "    sentences = file.read().splitlines()\n",
    "\n",
    "# Data overview\n",
    "print(\"Number of feedbacks in the dataset: \", len(sentences))\n",
    "print(\"First feedback: \", sentences[0])\n",
    "print(\"Last feedback: \", sentences[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First feedback:  slide giáo trình đầy đủ .\n",
      "Preprocessed feedback:  ['<s>', 'slide', 'giáo', 'trình', 'đầy', 'đủ', '</s>']\n"
     ]
    }
   ],
   "source": [
    "def preprocess(sentence):\n",
    "    ''' \n",
    "    Preprocess a sentence by tokenizing and adding start and end tokens also removing punctuation\n",
    "    '''\n",
    "    tokens = word_tokenize(sentence.lower()) # Tokenize\n",
    "    tokens = [token for token in tokens if token.isalpha()] # Remove punctuation\n",
    "    return ['<s>'] + tokens + ['</s>'] # Add start and end tokens\n",
    "\n",
    "# Demo preprocess \n",
    "print(\"First feedback: \", sentences[0])\n",
    "print(\"Preprocessed feedback: \", preprocess(sentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedback no.1: slide giáo trình đầy đủ .\n",
      "Feedback no.2: nhiệt tình giảng dạy , gần gũi với sinh viên .\n",
      "Feedback no.3: đi học đầy đủ full điểm chuyên cần .\n",
      "Trigram model:  {('<s>', 'slide', 'giáo'): 1, ('slide', 'giáo', 'trình'): 1, ('giáo', 'trình', 'đầy'): 1, ('trình', 'đầy', 'đủ'): 1, ('đầy', 'đủ', '</s>'): 1, ('<s>', 'nhiệt', 'tình'): 1, ('nhiệt', 'tình', 'giảng'): 1, ('tình', 'giảng', 'dạy'): 1, ('giảng', 'dạy', 'gần'): 1, ('dạy', 'gần', 'gũi'): 1, ('gần', 'gũi', 'với'): 1, ('gũi', 'với', 'sinh'): 1, ('với', 'sinh', 'viên'): 1, ('sinh', 'viên', '</s>'): 1, ('<s>', 'đi', 'học'): 1, ('đi', 'học', 'đầy'): 1, ('học', 'đầy', 'đủ'): 1, ('đầy', 'đủ', 'full'): 1, ('đủ', 'full', 'điểm'): 1, ('full', 'điểm', 'chuyên'): 1, ('điểm', 'chuyên', 'cần'): 1, ('chuyên', 'cần', '</s>'): 1}\n"
     ]
    }
   ],
   "source": [
    "def build_trigram_model(sentences):\n",
    "    '''\n",
    "    Build a trigram model from a list of sentences (counting the occurences of the trigrams)\n",
    "    Loop through each sentence and tokenize it, add count to trigram model\n",
    "    '''\n",
    "    trigram_model = {}\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = preprocess(sentence) # Tokenize the sentence\n",
    "\n",
    "        for i in range(len(tokens) - 2): # Loop through the tokens\n",
    "            trigram = (tokens[i], tokens[i+1], tokens[i+2])\n",
    "            if trigram in trigram_model:\n",
    "                trigram_model[trigram] += 1\n",
    "            else:\n",
    "                trigram_model[trigram] = 1\n",
    "        \n",
    "    return trigram_model\n",
    "\n",
    "# Demo trigram model\n",
    "for i in range(3):\n",
    "    print(f\"Feedback no.{i+1}: {sentences[i]}\")\n",
    "print(\"Trigram model: \", build_trigram_model(sentences[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  1924\n"
     ]
    }
   ],
   "source": [
    "def vocalbulary_size(sentences):\n",
    "    vocabulary = {}\n",
    "    for sentence in sentences:\n",
    "        vocab_token = preprocess(sentence)[1:-1] # Remove start and end tokens\n",
    "        for token in vocab_token:\n",
    "            if token in vocabulary:\n",
    "                vocabulary[token] += 1\n",
    "            else:\n",
    "                vocabulary[token] = 1\n",
    "    # print(vocabulary)\n",
    "    return len(vocabulary)\n",
    "\n",
    "# Demo vocabulary size\n",
    "print(\"Vocabulary size: \", vocalbulary_size(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedback no.2: em có học ở một trung tâm tiếng anh ở ngoài trường , thời lượng hợp lý , không nhiều không ít khoảng 2 - 3 tiết một buổi !\n",
      "Feedback no.3: em tiếp thu chậm .\n",
      "Feedback no.4: chia sẻ cho em nhiều điều hay .\n",
      "Unigram model:  {('<s>', 'chia'): 1, ('chia', 'sẻ'): 1, ('sẻ', 'cho'): 1, ('cho', 'em'): 1, ('em', 'nhiều'): 1, ('nhiều', 'điều'): 1, ('điều', 'hay'): 1, ('hay', '</s>'): 1, ('<s>', 'em'): 2, ('em', 'tiếp'): 1, ('tiếp', 'thu'): 1, ('thu', 'chậm'): 1, ('chậm', '</s>'): 1, ('em', 'có'): 1, ('có', 'học'): 1, ('học', 'ở'): 1, ('ở', 'một'): 1, ('một', 'trung'): 1, ('trung', 'tâm'): 1, ('tâm', 'tiếng'): 1, ('tiếng', 'anh'): 1, ('anh', 'ở'): 1, ('ở', 'ngoài'): 1, ('ngoài', 'trường'): 1, ('trường', 'thời'): 1, ('thời', 'lượng'): 1, ('lượng', 'hợp'): 1, ('hợp', 'lý'): 1, ('lý', 'không'): 1, ('không', 'nhiều'): 1, ('nhiều', 'không'): 1, ('không', 'ít'): 1, ('ít', 'khoảng'): 1, ('khoảng', 'tiết'): 1, ('tiết', 'một'): 1, ('một', 'buổi'): 1, ('buổi', '</s>'): 1}\n"
     ]
    }
   ],
   "source": [
    "def build_unigram_model(sentences):\n",
    "    '''\n",
    "    Build a unigram model from a list of sentences (counting the occurences of the unigrams)\n",
    "    Loop through each sentence and tokenize it, add count to unigram model\n",
    "    '''\n",
    "    unigram_model = {}\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = preprocess(sentence) # Tokenize the sentence\n",
    "\n",
    "        for i in range(len(tokens) - 1): # Loop through the tokens\n",
    "            unigram = (tokens[i], tokens[i+1])\n",
    "            if unigram in unigram_model:\n",
    "                unigram_model[unigram] += 1\n",
    "            else:\n",
    "                unigram_model[unigram] = 1\n",
    "        \n",
    "    return unigram_model\n",
    "\n",
    "# Demo trigram model\n",
    "for i in range(1, 4):\n",
    "    print(f\"Feedback no.{i+1}: {sentences[-i]}\")\n",
    "print(\"Unigram model: \", build_unigram_model(sentences[-3:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_count_model = build_trigram_model(sentences)\n",
    "unigram_count_model = build_unigram_model(sentences)\n",
    "vocabulary_size = vocalbulary_size(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentence_probability(sentence, unigram_count_model=unigram_count_model, trigram_count_model=trigram_count_model, vocalbulary_size=vocabulary_size):\n",
    "    start_time = time.time()\n",
    "    # For add one smoothing, need vocabulary size\n",
    "    tokens = preprocess(sentence)\n",
    "    probability = 1.0\n",
    "\n",
    "    for i in range(len(tokens) - 2):\n",
    "        if i == 0: # <s> + w1\n",
    "            # When its the starting word, we only need to calculate the P(w1|<s>) = Probability of w1 knowing that it is the first word\n",
    "            # We know that c(<s>) = number of sentences -> P(w1|<s>) = c(<s>, w1)/c(<s>) -> add-one smoothing = (c(<s>, w1) + 1)/(c(<s>) + V)\n",
    "            unigram = (tokens[i], tokens[i+1])\n",
    "            probability *= (unigram_count_model.get(unigram, 0) + 1)/(len(sentences) + vocalbulary_size)\n",
    "        else: # <s> + w1 + w2 or w1 + w2 + w3 or wn-1 + wn + </s>\n",
    "            # P(wn|wn-2, wn-1) = c(wn-2, wn-1, wn)/c(wn-2, wn-1) -> add-one smoothing = (c(wn-2, wn-1, wn) + 1)/(c(wn-2, wn-1) + V)\n",
    "            trigram = (tokens[i], tokens[i+1], tokens[i+2])\n",
    "            unigram = (tokens[i], tokens[i+1])\n",
    "            probability *= (trigram_count_model.get(trigram, 0) + 1)/(unigram_count_model.get(unigram, 0) + vocalbulary_size)\n",
    "    end_time = time.time()\n",
    "\n",
    "    return (probability, end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input feed back: \"slide đầy đủ chi tiết, thầy dạy nhiệt tình.\"\n",
      "Probability:  6.933514413952236e-23\n",
      "Execution time:  0.0\n",
      "Check time:  0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Input feed back: \\\"slide đầy đủ chi tiết, thầy dạy nhiệt tình.\\\"\")\n",
    "start_time = time.time()\n",
    "probability, execution_time = calculate_sentence_probability(\"slide đầy đủ chi tiết, thầy dạy nhiệt tình.\")\n",
    "end_time = time.time()\n",
    "print(\"Probability: \", probability)\n",
    "print(\"Execution time: \", execution_time)\n",
    "check_time = end_time - start_time\n",
    "print(\"Check time: \", check_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
